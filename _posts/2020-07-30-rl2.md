---
title:  "Mobile robot navigation using Reinforcement Learning"
search: true
categories: 
  - Reinforcement Learning
last_modified_at: 2020-07-30T08:06:00-05:00

---

So far, I have spent more than a week learning to work with the Deepbots framework, which helps to communicate Webots simulator with reinforcement learning algorithm training pipeline. This time the task was to teach a robot to navigate to any point in a workspace. Firstly, I decided to implement a navigation using only a discrete action space to get used to the working with RL, and will eventually switch to a continuous action space.

Through multiple iterations, I have come up with the following space dimensions:
* Observation space: 34 (2 position, 2 velocity, 30 laser)
* Action space: 4 (forward, turn left and right, stop)

For training I have used the Proximal Policy Optimization algorithm (again) with the following actor-critic network design, inspired by [2]:

![](/assets/images/posts/rl/Screenshot from 2020-07-30 14-40-42.png){: .align-center}

However, the most challenging part was the reward shaping. I've read a lot of papers reagarding mobile robot navigation using reinforcement learning, and implemented multiple of them, but with no success. The most notable case, for example, was the following navigation policy (I call it dancing robot policy):

{% include video id="hIVDhPR8zv8" provider="youtube" %}

The task was to reach a point with yellow ball, but robot is moving close to it (never actually reaching it, which will end the episode), to gather more dense rewards. This as a result gives a very strange kind of motion.

Finally, I used the reward model used by [2]:

![](/assets/images/posts/rl/Screenshot from 2020-07-30 14-51-36.png){: .align-center}

This reward shaping allowed to train the robot to reach a final destination in a very nice way, and took around 2 hours. The deployment of the generated policy is shown below:

{% include video id="-zyYEJ1bql0" provider="youtube" %}

The next task would be to train the robot to reach varying points (any) in the workspace, but it would definitely take much more time and compute resources.

[1] Tai, Lei, Giuseppe Paolo, and Ming Liu. "Virtual-to-real deep reinforcement learning: Continuous control of mobile robots for mapless navigation." 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2017.

[2] Botteghi, Nicol√≤, et al. "On Reward Shaping for Mobile Robot Navigation: A Reinforcement Learning and SLAM Based Approach." arXiv preprint arXiv:2002.04109 (2020).